"""Contains functions to parse and preprocess information from the input file"""
import sys
import os
import h5py
import logging
import multiprocessing as mp
import numpy as np
import pandas as pd
import pickle

from immunopepper.io_ import decode_utf8
from immunopepper.namedtuples import CountInfo
from immunopepper.namedtuples import GeneInfo
from immunopepper.namedtuples import GeneTable
from immunopepper.namedtuples import ReadingFrameTuple
from immunopepper.utils import encode_chromosome
from immunopepper.utils import find_overlapping_cds_simple
from immunopepper.utils import get_successor_list
from immunopepper.utils import leq_strand
from immunopepper.utils import pool_initializer
from immunopepper.utils import replace_I_with_L


def genes_preprocess_batch(genes, gene_idxs, gene_cds_begin_dict, all_read_frames=False):
    gene_info = []

    for gene in genes:
        gene.from_sparse()
        assert (gene.strand in ["+", "-"])
        assert (len(gene.transcripts) == len(gene.exons))

        # Ignore genes that have no CDS annotated in annotated frame mode
        if (not all_read_frames) and (gene.name not in gene_cds_begin_dict):
            gene_info.append(None)
            continue

        vertex_succ_list = get_successor_list(gene.splicegraph.edges, gene.splicegraph.vertices, gene.strand)
        if gene.strand == "+":
            vertex_order = np.argsort(gene.splicegraph.vertices[0, :])
        else:  # gene.strand=="-"
            vertex_order = np.argsort(gene.splicegraph.vertices[1, :])[::-1]

        # get the reading_frames
        reading_frames = {}
        vertex_len_dict = {}
        if not all_read_frames:
            for idx in vertex_order: # iterate over the exons in translational order
                reading_frames[idx] = set()
                v_start = gene.splicegraph.vertices[0, idx] # exon start
                v_stop = gene.splicegraph.vertices[1, idx] # exon end
                cds_begins = find_overlapping_cds_simple(v_start, v_stop, gene_cds_begin_dict[gene.name], gene.strand) # get a list of tuples for each annotated transcript
                vertex_len_dict[idx] = v_stop - v_start # add exon len the dict

                # Initialize reading regions from the CDS transcript annotations
                for cds_begin in cds_begins:
                    line_elems = cds_begin[2] # a list e.g. ['1', 'sim', 'CDS', '6061', '6980', '.', '+', '1', 'gene_id "gene10"; transcript_id "gene10.t1"; gene_type "protein_coding"; gene_name "gene10";']
                    cds_strand = line_elems[6] # strand
                    assert (cds_strand == gene.strand)
                    cds_phase = int(line_elems[7])
                    cds_left = int(line_elems[3])-1 # the same as cds_begin[0]
                    cds_right = int(line_elems[4]) # cds stop, same as cds_begin[1]

                    # TODO: need to remove the redundance of (cds_start, cds_stop, item)
                    if gene.strand == "-":
                        cds_right_modi = max(cds_right - cds_phase, v_start)
                        cds_left_modi = v_start
                        n_trailing_bases = cds_right_modi - cds_left_modi
                    else:
                        cds_left_modi = min(cds_left + cds_phase, v_stop)
                        cds_right_modi = v_stop
                        n_trailing_bases = cds_right_modi - cds_left_modi

                    read_phase = n_trailing_bases % 3
                    # add all reading frames from the annotation
                    reading_frames[idx].add(ReadingFrameTuple(cds_left_modi=cds_left_modi,
                                                              cds_right_modi=cds_right_modi,
                                                              read_phase=read_phase,
                                                              annotated_RF=True))
        gene.to_sparse()
        gene_info.append(GeneInfo(vertex_succ_list, vertex_order, reading_frames, vertex_len_dict, gene.splicegraph.vertices.shape[1]))

    return gene_info, gene_idxs


def genes_preprocess_all(genes, gene_cds_begin_dict, parallel=1, all_read_frames=False):
    """ Preprocess the gene and generate new attributes under gene object
        Modify the gene object directly

    Parameters
    ----------
    genes: List[Object]. List of gene objects. The object is generated by SplAdder
    gene_cds_begin_dict: Dict. str -> List(int) From gene name to list of cds start positions
    """

    if parallel > 1:
        global genes_info
        global cnt

    if os.path.exists('reading_frames.pickle'):
        genes_info = pickle.load(open('reading_frames.pickle', 'rb'))
        return genes_info

    if parallel > 1:
        genes_info = np.zeros((genes.shape[0],), dtype=object)
        cnt = 0

        def update_gene_info(result):
            global genes_info
            global cnt
            assert(len(result[0]) == len(result[1]))
            for i, tmp in enumerate(result[0]):
                if cnt > 0 and cnt % 1000 == 0:
                    sys.stdout.write('.')
                    if cnt % 10000 == 0:
                        sys.stdout.write('%i/%i\n' % (cnt, genes.shape[0]))
                    sys.stdout.flush()
                cnt += 1
                genes_info[result[1][i]] = tmp
            del result

        pool = mp.Pool(processes=parallel, initializer=pool_initializer)
        for i in range(0, genes.shape[0], 100):
            gene_idx = np.arange(i, min(i + 100, genes.shape[0]))
            _ = pool.apply_async(genes_preprocess_batch, args=(genes[gene_idx], gene_idx, gene_cds_begin_dict, all_read_frames,), callback=update_gene_info)
        pool.close()
        pool.join()
    else:
        genes_info = genes_preprocess_batch(genes, np.arange(genes.shape[0]), gene_cds_begin_dict, all_read_frames)[0]

    pickle.dump(genes_info, open('reading_frames.pickle', 'wb'), -1)

    return genes_info


def preprocess_ann(ann_path):
    """ Extract information from annotation file (.gtf, .gff and .gff3)

    Parameters
    ----------
    ann_path: str. Annotation file path

    Returns
    -------
    gene_table: NamedTuple.store the gene-transcript-cds mapping tables derived
        from .gtf file. has attribute ['gene_to_cds_begin', 'ts_to_cds', 'gene_to_ts']
    chromosome_set: set. Store the chromosome naming.
    """
    transcript_to_gene_dict = {}    # transcript -> gene id
    gene_to_transcript_dict = {}    # gene_id -> list of transcripts
    transcript_to_cds_dict = {}     # transcript -> list of CDS exons
    transcript_cds_begin_dict = {}  # transcript -> first exon of the CDS
    gene_cds_begin_dict = {}        # gene -> list of first CDS exons

    file_type = ann_path.split('.')[-1]
    chromosome_set = set()
    # collect information from annotation file
    for line in open(ann_path, 'r'):
        if line[0] == '#':
            continue
        item = line.strip().split('\t')
        chromosome_set.add(item[0])
        feature_type = item[2]
        attribute_item = item[-1]
        attribute_dict = attribute_item_to_dict(attribute_item, file_type, feature_type)
        # store relationship between gene ID and its transcript IDs
        if feature_type in ['transcript', 'mRNA']:
            gene_id = attribute_dict['gene_id']
            transcript_id = attribute_dict['transcript_id']
            if attribute_dict['gene_type'] != 'protein_coding' or attribute_dict['transcript_type'] != 'protein_coding':
                continue
            assert (transcript_id not in transcript_to_gene_dict)
            transcript_to_gene_dict[transcript_id] = gene_id
            if gene_id in gene_to_transcript_dict and transcript_id not in gene_to_transcript_dict[gene_id]:
                gene_to_transcript_dict[gene_id].append(transcript_id)
            else:
                gene_to_transcript_dict[gene_id] = [transcript_id]

        #TODO: python is 0-based (half-open) while gene annotation file(.gtf, .vcf, .maf) is one based inclusive
        elif feature_type == "CDS":
            parent_ts = attribute_dict['transcript_id']
            strand_mode = item[6]
            # cds_left and right will be START/STOP depending on the strand
            cds_left = int(item[3])-1   # 1-based inclusive, need to modify to get 0-based inclusive
            cds_right = int(item[4])    # inclusive in 1-based logic, so will be exclusive in 0-based --> OK
            frameshift = int(item[7])
            if parent_ts in transcript_to_cds_dict:
                transcript_to_cds_dict[parent_ts].append((cds_left, cds_right, frameshift))
            else:
                transcript_to_cds_dict[parent_ts] = [(cds_left, cds_right, frameshift)]
            if strand_mode == "+":
                cds_start, cds_stop = cds_left, cds_right
            else:
                cds_start, cds_stop = cds_right, cds_left

            # we only consider the start of the whole CoDing Segment
            if parent_ts not in transcript_cds_begin_dict or \
               leq_strand(cds_start, transcript_cds_begin_dict[parent_ts][0], strand_mode):
                transcript_cds_begin_dict[parent_ts] = (cds_start, cds_stop, item) # unmodifeied (1-based) left and right coordinates added (inside item)

    # collect first CDS exons for all transcripts of a gene
    for ts_key in transcript_to_gene_dict:
        target_gene = transcript_to_gene_dict[ts_key]
        if target_gene not in gene_cds_begin_dict:
            gene_cds_begin_dict[target_gene] = []
        if ts_key in transcript_cds_begin_dict:
            gene_cds_begin_dict[target_gene].append(transcript_cds_begin_dict[ts_key])

    # sort list of CDS exons per transcript
    for ts_key in transcript_to_cds_dict:
        transcript_to_cds_dict[ts_key] = sorted(transcript_to_cds_dict[ts_key], key=lambda coordpair: coordpair[0])

    genetable = GeneTable(gene_cds_begin_dict, transcript_to_cds_dict, gene_to_transcript_dict)
    return genetable, chromosome_set


def attribute_item_to_dict(a_item, file_type, feature_type):
    """  From attribute item in annotation file to get corresponding dictionary

    Parameters
    ----------
    a_item: str. attribute item
    file_type: str. Choose from {'gtf', 'gff', 'gff3'}
    feature_type: str. Extract other fields. We only
        consider 'CDS', 'mRNA' and 'transcript'

    Returns
    -------
    gtf_dict: dict. store all the necessary data

    """
    gtf_dict = {}
    if file_type.lower() == 'gtf':
        attribute_list = a_item.split('; ')
        for attribute_pair in attribute_list:
            pair = attribute_pair.split(' ')
            gtf_dict[pair[0]] = pair[1][1:-1]
    elif file_type.lower() == 'gff3':
        attribute_list = a_item.split(';')
        for attribute_pair in attribute_list:
            pair = attribute_pair.split('=')
            gtf_dict[pair[0]] = pair[1]
    elif file_type.lower() == 'gff':
        gff_dict = {}
        attribute_list = a_item.split(';')
        for attribute_pair in attribute_list:
            pair = attribute_pair.split('=')
            gff_dict[pair[0]] = pair[1]  # delete "", currently now work on level 2
        if feature_type == 'CDS':
            gtf_dict['transcript_id'] = gff_dict['Parent']
        elif feature_type in {'mRNA', 'transcript'}:  # mRNA or transcript
            gtf_dict['gene_id'] = gff_dict['geneID']
            gtf_dict['transcript_id'] = gff_dict['ID']
            gtf_dict['gene_type'] = gff_dict['gene_type']
            gtf_dict['transcript_type'] = gff_dict['transcript_type']

    return gtf_dict

# edge_idxs are segment, not exon junctions (based on test data)
def search_edge_metadata_segmentgraph(gene, kmer_path, edge_idxs=None, edge_counts=None):
    """
    Traverses a path of segments (kmer_path) and retrieves the expression counts for each junction (edge) 
    if it exists in the provided edge index and count arrays.

    Args:
        gene: An object representing a gene, expected to have a 'segmentgraph' attribute.
        kmer_path (list): A list of tuples or identifiers representing the path of segments (k-mers) to traverse.
        edge_idxs (np.ndarray, optional): Array of flattened indices representing valid edges (junctions) in the segment graph.
        edge_counts (np.ndarray, optional): Array of expression counts for each edge, shape (num_edges, num_samples) or (num_edges,).
            both from the SplAdder count file ['edges'] and ['edge_idx'].
    Returns:
        tuple:
            - edges_res_metafile (float or np.nan): Placeholder for edge metadata file (currently unused, always np.nan).
            - edges_res (np.ndarray): Array of expression counts for the junctions found along the k-mer path.
              Shape is (num_junctions_found, num_samples) or (0, num_samples) if no junctions are found.
    """
    
    def get_segmentgraph_edge_expr_kmer(seg_id1, seg_id2, segmentgraph, edge_idxs, edge_counts):
        """
        Retrieve the expression counts for a specific edge (junction) between two segments in a segment graph.
        Args:
            seg_id1 (int): The ID of the first segment (source).
            seg_id2 (int): The ID of the second segment (target).
            segmentgraph: An object representing the segment graph, expected to have a 'seg_edges' attribute (2D array-like).
            edge_idxs (np.ndarray): 1D array of flattened indices representing valid edges in the segment graph.
            edge_counts (np.ndarray): Array of expression counts for each edge. Can be 1D (single sample) or 2D (multi-sample).
        Returns:
            np.ndarray or None: The expression counts for the specified edge as a 1D array (length = number of samples),
            or None if the edge does not exist or is not found in the expression data.
        """

        # Check the segments are connected by an intron (the junction exists)
        if not segmentgraph.seg_edges[seg_id1, seg_id2]:
            return None  # No junction between these segments
        
        # Create junction index in the segment graph edge matrix (idx is the value found in edge_idxs)
        idx = np.ravel_multi_index([seg_id1, seg_id2], segmentgraph.seg_edges.shape)
        
        # Find this junction in the edge_idxs array (find the position for the value in edge_idxs)
        cidx = np.searchsorted(edge_idxs, idx)
        
        # Validate that we found the correct edge
        if cidx >= len(edge_idxs) or edge_idxs[cidx] != idx:
            return None  # Edge not found in expression data
        
        # Extract expression counts
        if len(edge_counts.shape) > 1:
            counts = edge_counts[cidx, :]  # Multi-sample case
        else:
            counts = np.array([edge_counts[cidx]])  # Single sample case
            
        return counts

    edges_res_metafile = np.nan
    
    # Handle cases where we don't have junction data
    if edge_idxs is None or edge_counts is None or len(kmer_path) < 2:
        # Return empty array with appropriate shape
        out_shape = edge_counts.shape[1] if (edge_counts is not None and len(edge_counts.shape) > 1) else 1
        return edges_res_metafile, np.zeros((0, out_shape), dtype='float')
    
    segmentgraph = gene.segmentgraph
    junction_expressions = []
    kmer_path = sorted(kmer_path) # reverse order to  gurantee correct matrix indexing for both strands
    
    # Process each potential junction in the k-mer path
    for i in range(len(kmer_path) - 1):
        seg_id1 = kmer_path[i][0]      # Current segment ID
        seg_id2 = kmer_path[i + 1][0]  # Next segment ID
        
        # Get expression for this junction (if it exists)
        junction_expr = get_segmentgraph_edge_expr_kmer(seg_id1, seg_id2, segmentgraph, edge_idxs, edge_counts)
        
        # Only add if there's an actual junction
        if junction_expr is not None:
            junction_expressions.append(junction_expr)
    
    # Stack all junction expressions
    if junction_expressions:
        edges_res = np.stack(junction_expressions)
    else:
        # No junctions found - k-mer spans only adjacent segments in same exon
        out_shape = edge_counts.shape[1] if len(edge_counts.shape) > 1 else 1
        edges_res = np.zeros((0, out_shape), dtype='float')
    
    return edges_res_metafile, edges_res


def parse_gene_metadata_info(h5fname, sample_list):
    """ Parse the count file

    Parameters
    ----------
    h5fname: str. .count.h5f file
    sample_list: List(str). List of samples.

    Returns
    -------
    countinfo: Namedtuple. Stores all count information. Has attributes:
        'sample_idx_dict' --> dict from sample name to index
        'gene_idx_dict' --> dict from gene name to index
        'gene_ids_segs' --> array containing segment-geneID relation
        'gene_ids_edges' --> array containing edge-geneID relation
        'h5fname' --> HDF5 file name
    """

    # the SplAdder count hdf5 file has the following structure
    #   h5f["strains"] --> sample names
    #   h5f["segments"] --> segment expression (rows: segments, columns: samples)
    #   h5f["edges"] --> edge expression (rows: edges, columns: samples)
    #   h5f["edge_idx"] --> multi-row index encoding the edge in the splice graph (rows:
    h5f = h5py.File(h5fname, 'r')
    assert (h5f["strains"].shape[0] == h5f["segments"].shape[1])
    assert (h5f["gene_ids_segs"].size == h5f["segments"].shape[0])
    assert (h5f["gene_ids_edges"].size == h5f["edges"].shape[0])

    # create a sample name dictionary mapping sample names to indices
    count_names = h5f['strains'][:] if len(h5f['strains'].shape) == 1 else h5f['strains'][:, 0]
    sample_idx_dict = dict([(n.decode('utf8'), i) for i, n in enumerate(count_names)])

    # create a gene name dictionary mapping gene names to indices
    gene_names = h5f['gene_names'][:] if len(h5f['gene_names'].shape) == 1 else h5f['gene_names'][:, 0]
    gene_idx_dict = dict([(n.decode('utf8'), i) for i, n in enumerate(gene_names)])

    gene_ids_segs = h5f['gene_ids_segs'][:] if len(h5f['gene_ids_segs'].shape) == 1 else h5f['gene_ids_segs'][:, 0]
    gene_ids_edges = h5f['gene_ids_edges'][:] if len(h5f['gene_ids_edges'].shape) == 1 else h5f['gene_ids_edges'][:, 0]

    # segs
    gene_ids_segs_u, gene_ids_segs_idx = np.unique(gene_ids_segs, return_index=True)
    gene_ids_segs_idx_last = np.r_[gene_ids_segs_idx[1:], [gene_ids_segs.shape[0]]]
    gene_id_to_segrange = dict()
    for i, g in enumerate(gene_ids_segs_u):
        gene_id_to_segrange[g] = (gene_ids_segs_idx[i], gene_ids_segs_idx_last[i])
    # edges
    gene_ids_edges_u, gene_ids_edges_idx = np.unique(gene_ids_edges, return_index=True)
    gene_ids_edges_idx_last = np.r_[gene_ids_edges_idx[1:], [gene_ids_edges.shape[0]]]
    gene_id_to_edgerange = dict()
    for i, g in enumerate(gene_ids_edges_u):
        gene_id_to_edgerange[g] = (gene_ids_edges_idx[i], gene_ids_edges_idx_last[i])

    countinfo = CountInfo(sample_idx_dict,
                          gene_idx_dict,
                          gene_id_to_segrange,
                          gene_id_to_edgerange,
                          h5fname)
    h5f.close()
    if sample_list:
        # Retrieve count id matching input samples
        matching_count_ids = np.array([s_idx for input_sample in sample_list for s_idx, graph_sample in enumerate(count_names)
                                       if graph_sample.decode() == input_sample])
        if countinfo is not None and len(matching_count_ids) == 0:
            logging.error("Output samples do not match count file samples")
            sys.exit(1)
    else:
        matching_count_ids = None

    matching_count_samples = [n.decode('utf8') for n in count_names]
    return countinfo, matching_count_samples, matching_count_ids


# TODO(dd): move mutation parsing methods to mutations.py after review; left here to make code review easier
def parse_mutation_from_vcf(vcf_path: str, mutation_mode: str, mutation_sample: str = None,
                            graph_to_mutation_samples=dict[str, str], heter_code: int = 0, output_dir: str = None):
    """Extract mutation information from the given vcf or vcf.h5 file
    :param vcf_path: path to the VCF file to be read
    :param mutation_mode: what mutations to apply to the reference. One of 'ref', 'germline', 'somatic',
        'somatic_and_germline'
    :param mutation_sample: TODO(dd) - clarify inconsistency with VCF/HD5 method
    :param graph_to_mutation_samples: dictionary mapping sample id names in the Spladder graph/count files to
        sample ids in the mutation files
    :param heter_code: Can be 0 or 2. specify which number represents the heterozygous allele (HDF5 VCF only)
        0: 0-> homozygous alternative(1|1), 1-> heterozygous(0|1,1|0) 2->homozygous reference(0|0)
        2: 0-> homozygous reference(0|0), 1-> heterozygous(0|1,1|0) 2->homozygous alternative(1|1)
    :param output_dir: location where the pickled result will be cached, if not None
    :return: a dictionary mapping (sample, chromosome) pairs to mutation data, where mutation data is
        a dictionary mapping mutation position, to mutation properties e.g.::
        {('sample1', 'X'): {111: {'ref_base': 'T', 'mut_base': 'C', 'strand': '-',
            'variant_Classification': 'Silent', 'variant_Type': 'SNP'}}}
    :rtype: dict[(str,str):dict[int, dict[str:str]]]
    """

    mutation_to_graph_samples = {file_sample: target_sample for target_sample, file_sample
                                 in graph_to_mutation_samples.items()}
    if output_dir is not None:
        vcf_pkl_file = os.path.join(output_dir, f'{mutation_mode}_vcf.pickle')
        if os.path.exists(vcf_pkl_file):
            f = open(vcf_pkl_file, 'rb')
            mutation_dict = pickle.load(f)
            pickled_sample_ids = {sample_id for sample_id, chrm in mutation_dict}
            _check_mutation_sample_presence(mutation_sample, pickled_sample_ids, graph_to_mutation_samples)
            logging.info(f'Using pickled VCF mutation dict in: {vcf_pkl_file} instead of loading data from: {vcf_path}')
            return mutation_dict

    if not os.path.exists(vcf_path):
        logging.error(f'Could not find mutation file: {vcf_path}')
        sys.exit(1)

    file_type = vcf_path.split('.')[-1]

    if file_type == 'h5':  # hdf5 file
        mutation_dict = parse_mutation_from_vcf_h5(vcf_path, mutation_sample, heter_code,
                                                   graph_to_mutation_samples)
        # TODO(reviewers): why no pickle created?
        logging.info(f'Read germline mutation dict from h5 file in {vcf_path}. No pickle file created')
        return mutation_dict

    # vcf text file
    lines = open(vcf_path, 'r').readlines()
    mutation_dict = {}
    for line in lines:
        if line.strip()[:2] == '##':  # annotation line
            continue
        if line.strip()[0] == '#':  # head line
            fields = line.strip().split('\t')
            vcf_sample_set = fields[9:]
            _check_mutation_sample_presence(mutation_sample, vcf_sample_set, graph_to_mutation_samples)
            continue
        items = line.strip().split('\t')
        chromosome = items[0]
        pos = int(items[1]) - 1
        var_dict = {'ref_base': items[3],
                    'mut_base': items[4],
                    'qual': items[5],
                    'filter': items[6]}
        if len(var_dict['ref_base']) == len(var_dict['mut_base']):  # only consider snp for now
            for i, file_sample in enumerate(vcf_sample_set):
                if items[9 + i].split(':')[0] in {'1|1', '1|0', '0|1', '0/1', '1/0', '1/1'}:
                    if file_sample not in mutation_to_graph_samples:
                        mutation_to_graph_samples[file_sample] = file_sample
                    if (mutation_to_graph_samples[file_sample], chromosome) in mutation_dict.keys():
                        mutation_dict[(mutation_to_graph_samples[file_sample], chromosome)][int(pos)] = var_dict
                    else:
                        mutation_dict[(mutation_to_graph_samples[file_sample], chromosome)] = {int(pos): var_dict}
    if output_dir is not None:
        f_pkl = open(vcf_pkl_file, 'wb')
        pickle.dump(mutation_dict, f_pkl)
        logging.info(f'Cached contents of {vcf_path} to {vcf_pkl_file}')

    return mutation_dict


def parse_mutation_from_vcf_h5(h5_vcf_path: str, mutation_sample: str, heter_code: int = 0,
                               graph_to_mutation_samples=dict[str, str]):
    """
    Extract germline mutation information from given vcf h5py file.

    Parameters and return value: see :meth:`parse_mutation_from_vcf`
    """
    a = h5py.File(h5_vcf_path, 'r')

    mut_dict = {}

    vcf_sample_set = [decode_utf8(item) for item in a['gtid']]
    _check_mutation_sample_presence(mutation_sample, vcf_sample_set, graph_to_mutation_samples)
    col_id = vcf_sample_set.index(mutation_sample)
    # the 'gt' column stores the most likely genotype for the sample
    row_id = np.where(np.logical_or(a['gt'][:, col_id] == heter_code, a['gt'][:, col_id] == 1))[0]

    for irow in row_id:
        chromosome = encode_chromosome(a['pos'][irow, 0])
        pos = a['pos'][irow, 1] - 1
        mut_base = decode_utf8(a['allele_alt'][irow])
        ref_base = decode_utf8(a['allele_ref'][irow])
        var_dict = {'mut_base': mut_base, 'ref_base': ref_base}
        if (mutation_sample, chromosome) in mut_dict:
            mut_dict[(mutation_sample, chromosome)][pos] = var_dict
        else:
            mut_dict[(mutation_sample, chromosome)] = {pos: var_dict}
    return mut_dict


def parse_mutation_from_maf(maf_path: str, mutation_mode: str, mutation_sample: str,
                            graph_to_mutation_samples: dict[str, str], output_dir: str = None):
    """
    Extract somatic mutation information from the given MAF file.
    :param mutation_mode: what mutations to apply to the reference.
        One of 'ref', 'germline', 'somatic', 'somatic_and_germline'
    :param mutation_sample:
    :param maf_path: path to the MAF file to be read
    :param output_dir: if not None, directory to cache a pickled version of the mutation dict
    :param graph_to_mutation_samples: dictionary mapping sample id names in the Spladder graph/count files to
        sample ids in the mutation files
    :return: a dictionary mapping (sample, chromosome) pairs to mutation data, where mutation data is
        a dictionary mapping mutation position, to mutation properties e.g.::
        {('sample1', 'X'): {111: {'ref_base': 'T', 'mut_base': 'C', 'strand': '-',
            'variant_Classification': 'Silent', 'variant_Type': 'SNP'}}}
    :rtype: dict[(str,str):dict[int, dict[str:str]]]
    """

    mutation_to_graph_samples = {file_sample: target_sample for target_sample, file_sample
                                 in graph_to_mutation_samples.items()}

    if output_dir:
        maf_pkl_file = os.path.join(output_dir, f'{mutation_mode}_maf.pickle')
        if os.path.exists(maf_pkl_file):  # load pickled mutation dictionary
            f = open(maf_pkl_file, 'rb')
            mutation_dict = pickle.load(f)
            logging.info(f'Using pickled MAF mutation dict in: {maf_pkl_file} instead of loading data from: {maf_path}')
            maf_sample_set = {sample_id for sample_id, chrm in mutation_dict}
            _check_mutation_sample_presence(mutation_sample, maf_sample_set, graph_to_mutation_samples)
            return mutation_dict

    if not os.path.exists(maf_path):
        logging.error(f'Could not find mutation file: {maf_path}')
        sys.exit(1)

    lines = open(maf_path).readlines()
    mutation_dict = {}
    maf_sample_set = set()
    for i, line in enumerate(lines[1:]):
        items = line.strip().split('\t')
        if items[9] == 'SNP':  # only consider snp
            file_sample = items[15]
            maf_sample_set.add(file_sample)
            chromosome = items[4]
            pos = int(items[5]) - 1
            var_dict = {
                'ref_base': items[10],
                'mut_base': items[12],
                'strand': items[7],
                'variant_Classification': items[8],
                'variant_Type': items[9]}
            if file_sample not in mutation_to_graph_samples:
                mutation_to_graph_samples[file_sample] = file_sample
            if (mutation_to_graph_samples[file_sample], chromosome) in mutation_dict:
                mutation_dict[(mutation_to_graph_samples[file_sample], chromosome)][int(pos)] = var_dict
            else:
                mutation_dict[(mutation_to_graph_samples[file_sample], chromosome)] = {int(pos): var_dict}

    if output_dir:
        f_pkl = open(maf_pkl_file, 'wb')
        pickle.dump(mutation_dict, f_pkl)
        logging.info(f'Cached contents of {maf_path} to {maf_pkl_file}')

    _check_mutation_sample_presence(mutation_sample, maf_sample_set, graph_to_mutation_samples)
    return mutation_dict


def _check_mutation_sample_presence(mutation_sample, maf_or_vcf_sample_set, graph_to_mutation_samples):
    if mutation_sample and graph_to_mutation_samples[mutation_sample] not in maf_or_vcf_sample_set:
        logging.error(f"Target mutation sample {graph_to_mutation_samples[mutation_sample]} "
                      f"({mutation_sample} in graph/count) is not found in mutation/variant file."
                      f" Please check --mutation-sample or consider using --sample-name-map.")
        logging.error(f"Samples in mutation/variant file are: {maf_or_vcf_sample_set}")
        sys.exit(1)
    if graph_to_mutation_samples:
        for graph_name, mutation_name in graph_to_mutation_samples.items():
            if mutation_name not in maf_or_vcf_sample_set:
                logging.error(f"Sample {mutation_name} ({graph_name} in graph/count) to extract"
                              f" and pickle is not found in mutation/variant file. "
                              f" Please check --pickle-samples or consider using --sample-name-map.")
                logging.error(f"Samples in mutation/variant file are: {maf_or_vcf_sample_set}")
                sys.exit(1)


# todo: support tsv file in the future
def parse_junction_meta_info(h5f_path):
    """ Extract introns of interest from given h5py file

    Parameters
    ----------
    h5f_path: str, h5py file path

    Returns
    -------
    junction_dict: dict, key (chromosome id), value (set of coordinate pairs)

    """
    if h5f_path is None:
        return None
    else:
        h5f = h5py.File(h5f_path, 'r')
        chrms = h5f['chrms'][:]
        pos = h5f['pos'][:].astype('str')
        strand = h5f['strand'][:]
        junction_dict = {}

        for i, ichr in enumerate(chrms):
            try:
                junction_dict[decode_utf8(ichr)].add(':'.join([pos[i, 0], pos[i, 1], decode_utf8(strand[i])]))
            except KeyError:
                junction_dict[decode_utf8(ichr)] = set([':'.join([pos[i, 0], pos[i, 1], decode_utf8(strand[i])])])
    return junction_dict


def parse_uniprot(uniprot_path):

    if uniprot_path:
        uniprot = set(pd.read_csv(uniprot_path, header=None)[0])
        return set(map(replace_I_with_L, uniprot))
    else:
        return None


def parse_gene_choices(genes_interest, process_chr, process_num, complexity_cap, disable_process_libsize, graph_data):

    if process_num == 0:  # Default process all genes
        num = len(graph_data)
    else:
        num = process_num
        if num > len(graph_data):
            logging.error(
                "Requested more genes than available in splice graph. Check argument --process_num")
            sys.exit(1)
        graph_data = graph_data[:num]
        disable_process_libsize = True
        logging.warning(
            "Developer mode, processing the first {} genes. Library size will not be outputted.".format(process_num))

    if genes_interest is not None:
        genes_interest = pd.read_csv(genes_interest, header=None)[0].tolist()
        if len(np.array([gene for gene in graph_data if gene.name in genes_interest])) == 0:
            logging.error("Gene of interest not found in splicing graph. Check argument --genes_interest")
            sys.exit(1)
    else:
        genes_interest = [gene.name for gene in graph_data]

    if process_chr is not None:
        gene_with_chr = [gene.name for gene in graph_data if gene.chr in process_chr]
        if len(gene_with_chr) == 0:
            logging.error(
                "Chromosome {} not found in splicing graph. Check argument --process_chr".format(
                    process_chr))
            sys.exit(1)
        genes_interest = [gene for gene in genes_interest if gene in gene_with_chr]
        if len(genes_interest) == 0:
            logging.error(
                "Gene of interest and chromosome of interest do not match. Check argument --genes_interest, --process_chr")
            sys.exit(1)

    if complexity_cap is None or complexity_cap == 0:  # Default no complexity cap
        complexity_cap = np.inf

    return graph_data, genes_interest, num, complexity_cap, disable_process_libsize


def parse_output_samples_choices(arg, countinfo, matching_count_ids, matching_count_samples):
    """handle output_sample relatively to output mode"""
    process_output_samples = ['cohort']
    if countinfo:
        # If output samples requested, look for sample ids in count file
        if arg.output_samples:
            arg.output_samples = np.array(arg.output_samples)[np.argsort(matching_count_ids)]
            arg.output_samples = [output_sample.replace('-', '').replace('_', '').replace('.', '').replace('/', '')
                                  for output_sample in arg.output_samples]
            output_samples_ids = matching_count_ids[np.argsort(matching_count_ids)]
        # If no output samples requested, take all samples in countfile
        else:
            arg.output_samples = [output_sample.replace('-', '').replace('_', '').replace('.', '').replace('/', '')
                                  for output_sample in matching_count_samples]
            output_samples_ids = None
    else:
        output_samples_ids = None
        if arg.output_samples:
            logging.error(
                "--output-samples were specified but no --count-path was provided. Cannot extract desired sample expression.")
            sys.exit(1)

    return process_output_samples, output_samples_ids
